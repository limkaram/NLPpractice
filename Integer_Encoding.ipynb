{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Integer_Encoding.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMo16OuvIkuIHnZWxeXDEAf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limkaram/Natural_language_processing_with_deep_learning/blob/main/Integer_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ms9f7VfqIg"
      },
      "source": [
        "## 정수 인코딩(Integer Encoding)\n",
        "* 보통 단어 빈도수가 높은 순으로 내림차순 정렬한 후 정수 인코딩을 진행하는 경우가 많음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTQfOJAjgFxq"
      },
      "source": [
        "1. Dictionary 활용 정수 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9EcbroLfrwz"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTah8XFkkOCt"
      },
      "source": [
        "# 텍스트 정제 단계 : 불용어 제거, 길이 2 이하 단어 제거\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "vocabulary = []\n",
        "for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    cleaned_words = []\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        if word not in stop_words:\n",
        "            if len(word) >= 2:\n",
        "                cleaned_words.append(word)\n",
        "    vocabulary.append(cleaned_words)\n",
        "print(vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGRu9xLblpeR"
      },
      "source": [
        "# dictionary '단어' : 빈도수 형태로 변환\n",
        "vocap = {}\n",
        "\n",
        "for vocaps in vocabulary:\n",
        "    for v in vocaps:\n",
        "        if v in vocap:\n",
        "            vocap[v] += 1\n",
        "        else:\n",
        "            vocap[v] = 1\n",
        "\n",
        "vocaps = sorted(vocap.items(), key=lambda x: x[1], reverse=True) # 내림차순 정렬\n",
        "print(vocaps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfxK8PLJmqrn"
      },
      "source": [
        "# 정렬된 문자들에 인덱스 부여\n",
        "word_to_index = {}\n",
        "count = 0\n",
        "for word, frequency in vocaps:\n",
        "    if frequency > 1:  # 빈도수 2 이하 제거\n",
        "        count += 1\n",
        "        word_to_index[word] = count\n",
        "word_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgF4UHKpe1t8"
      },
      "source": [
        "# 상위 빈도수 5개의 단어만 사용시\n",
        "vocab_size = 5\n",
        "\n",
        "words_frequency = [word for word, idx in word_to_index.items() if idx >= vocab_size + 1]\n",
        "words_frequency\n",
        "\n",
        "for word in words_frequency:\n",
        "    del word_to_index[word]\n",
        "\n",
        "# word_to_index를 상위 빈도수 5개의 단어로 축소하였으므로, 이에 포함되지 않는 단어는 'OOV(Out-Of_Vocabulary)'처리를 위해 OOV 추가\n",
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "word_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwc68j_IgPyV"
      },
      "source": [
        "# vocab_list : 문장 토큰화 후 각 문장들을 정제 후 단어 토큰화 시킨 결과 모음\n",
        "encoded = []\n",
        "for vocab in vocabulary:\n",
        "    temp = []\n",
        "    for word in vocab:\n",
        "        if word not in word_to_index:\n",
        "            temp.append(word_to_index['OOV'])\n",
        "            continue\n",
        "        temp.append(word_to_index[word])\n",
        "    encoded.append(temp)\n",
        "encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7-osxcTxff2"
      },
      "source": [
        "2. Counter 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL7a7JwIxh-8"
      },
      "source": [
        "# from collections import Counter\n",
        "\n",
        "words = sum(vocabulary, [])\n",
        "\n",
        "counter = Counter(words)\n",
        "vocab_size = 5\n",
        "vocab = counter.most_common(vocab_size)\n",
        "\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for word, frequency in vocab:\n",
        "    i = i+1\n",
        "    word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBwoPOP00u81"
      },
      "source": [
        "3. Keras 텍스트 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbnU9Fl30xyj"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)  # 입력한 텍스트로부터 단어 빈도수가 높은 순으로 낮은 정수 인덱스 부여\n",
        "tokenizer.word_index  # 단어에 부여된 인덱스 확인\n",
        "tokenizer.word_counts  # 단어별 빈도수 확인\n",
        "tokenizer.texts_to_sequences(sentences)  # 입력으로 들어온 코퍼스에 대해 각 단어를 정해놓은 인덱스로 변환"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwa2SdBo13oH"
      },
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # num_words 옵션 : 상위 5개 단어만 사용(Counter의 most_common과 동일)\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYyCy-zp1_Qc"
      },
      "source": [
        "* num_words는 숫자를 0부터 카운트를 하므로 5개의 단어에 인덱스를 각각 부여했다면, 1~5까지의 인덱스 영역이 존재하고, 이를 반영해주기 위해선 num_words를 6으로해줘야 0~5까지 인덱스를 확인함\n",
        "* 숫자 0에는 지정된 단어가 존재하지 않는데도 keras tokenizer 숫자까지 단어 집합의 크기로 산정하는 이유는 padding 때문"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXPQaSSp2vGY"
      },
      "source": [
        "tokenizer.word_index\n",
        "tokenizer.word_counts\n",
        "tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KEBEVvA22tX"
      },
      "source": [
        "* tokenizer = Tokenizer(num_words = vocab_size + 1)를 적용해 주었음에도 tokenizer.word_index와 tokenizer.word_counts 출력은 인덱스가 1~5까지가 아닌 모든 단어를 출력함\n",
        "* 신제 적용은 texts_to_sequences()에 적용이 되어 출력됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw5l9JXl3hAm"
      },
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
        "# 빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYnz0mit3nei"
      },
      "source": [
        "* 케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 아예 단어를 제거한다는 특징을 가짐\n",
        "* 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용"
      ]
    }
  ]
}