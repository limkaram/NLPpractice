{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One_Hot_Encoding.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMnaD8j59ta+qoESjXGoCMw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limkaram/Natural_language_processing_with_deep_learning/blob/main/One_Hot_Encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xMzvz9o9eNy"
      },
      "source": [
        "## 원-핫 인코딩(One-Hot Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uejaRIpb9lkJ"
      },
      "source": [
        "* vocabulary(단어 집합) : 서로 다른 단어들의 집합\n",
        "* books와 book을 서로 다른 단어로 간주함\n",
        "* 텍스트 내의 모든 단어를 중복을 허용하지 않고 모아놓은 것을 vocabulary라고 함\n",
        "* one hot encoding 과정\n",
        "    - 각 단어에 고유한 인덱스 부여\n",
        "    - 표현하고 싶은 단어 인덱스 1부여 다른 단어의 인덱스 위치 0부여"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoFcn9E5AR7a"
      },
      "source": [
        "1. 직접 One-Hot Encoding 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSaP5WeX-Y9H"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eMImnLa9jZc"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "okt = Okt()\n",
        "token = okt.morphs('나는 자연어 처리를 배운다')\n",
        "token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI52Z8Hk-jqY"
      },
      "source": [
        "# Integer Encoding\n",
        "word2index = {}\n",
        "\n",
        "for voca in token:\n",
        "    if voca not in word2index:\n",
        "        word2index[voca] = len(word2index)\n",
        "word2index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvdPTqUx_CgA"
      },
      "source": [
        "# one-hot encoding definition\n",
        "\n",
        "def one_hot_encoding(word, word2index):  # word(token)을 입력하면, one-hot encoding 결과를 주는 함수\n",
        "    one_hot_vector = [0] * len(word2index)\n",
        "    one_hot_vector[word2index[word]] = 1\n",
        "    return one_hot_vector\n",
        "\n",
        "one_hot_encoding('자연어', word2index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inf6Kbo1AY8g"
      },
      "source": [
        "2. keras 활용 one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_rGp3dRAg7O"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])  # git_on_texs 입력은 리스트로\n",
        "\n",
        "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
        "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
        "encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59fmrZJnB922"
      },
      "source": [
        "one_hot = to_categorical(encoded)\n",
        "one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz60KdLHCFFq"
      },
      "source": [
        "* 원-핫 인코딩은 단어 개수가 늘어날 수록 벡터를 저장하기 위해 필요한 공간이 계속 늘어나는 단점 존재(벡터 차원이 계속 증가)\n",
        "* 원-핫 벡터는 단어 집합의 크기가 곧 벡터의 차원수\n",
        "* 단어간 유사도를 표현하지 못함\n",
        "* 해결 방안으로 단어간 잠재 의미를 반영하여 다차원 공간에서 벡터화하는 기법이 등장\n",
        "    - 카운트 기반 벡터화 : LSA, HAL\n",
        "    - 예측 기반 벡터화 : NNLM, RNNLM, Word2Vec, FastText\n",
        "    - 카운트 + 예측 기반 : GloVe"
      ]
    }
  ]
}