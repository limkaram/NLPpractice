{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzi+4HOKOkGcCIMmTePmbb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limkaram/Natural_language_processing_with_deep_learning/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfOPzN7Zd3ZU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# 데이터 파일 경로 : r'/content/gdrive/My Drive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxdMIj6PFS4a"
      },
      "source": [
        "## 1. Word Tokenization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siuraZvMffG6"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdPvoQJcIJTi"
      },
      "source": [
        "* keras는 알파벳을 소문자로 바꾸면서 온점이나 컴마, 느낌표 등의 구두점을 제거하고 아포스트로피는 보존함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k3q-DD9ISj7"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUtzYqDFIspV"
      },
      "source": [
        "* 표준 토큰화 방법 중 하나인 Penn Treebank Tokenization\n",
        "    - 규칙 1. 하이푼(-)으로 구성된 단어는 하나로 유지\n",
        "    - 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PySgudiAWlAA"
      },
      "source": [
        "## 2. Sentence Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCsQaN1OJbXS"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to mae sure no one was near.\"\n",
        "nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht1yb6ltKA4Z"
      },
      "source": [
        "* 온점을 기준으로 문장을 구분하지 않음을 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcJYJiuTKZ7w"
      },
      "source": [
        "!pip install kss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPdtG3d0KMYC"
      },
      "source": [
        "import kss\n",
        "\n",
        "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
        "kss.split_sentences(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaDKvXbCKf5M"
      },
      "source": [
        "* 한국어 sentence tokenize 패키지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkzI3A63Wsik"
      },
      "source": [
        "## 3. 품사 태깅 및 형태소 분석기 활용\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ke5KCGXS-yp"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tag(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDSqN4hTToLb"
      },
      "source": [
        "* 영어 품사 태깅 방법\n",
        "* Penn Treeback POG Tags 라는 기준을 활용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtYBrJXFUBCE"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5ovPRsGT1-X"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "okt.morphs(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E2RohT3UQKN"
      },
      "source": [
        "* 한국어 토큰화 과정\n",
        "* Okt 형태소 분석기는 Twitter와 동일 한 것(개명됨)\n",
        "* morphs() : 형태소 단위로 토큰화\n",
        "* nouns() : 명사에 해당하는 형태소만 반환\n",
        "* pos() : 품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq8ar2NYUxRe"
      },
      "source": [
        "from konlpy.tag import Kkma\n",
        "\n",
        "text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
        "kkma = Kkma()\n",
        "tokens = kkma.morphs(text)\n",
        "kkma.pos(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3LfxDFdU5GG"
      },
      "source": [
        "* 타 형태소분석기 또한 같은 메소드를 가지고 있음\n",
        "* mecab 형태분석기는 속도측면에서 빠름"
      ]
    }
  ]
}